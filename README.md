Glosario 
	¿Quién lo creo?	¿Cuándo?	¿Para qué sirve?
Hadoop 	Doug Cutting
	1 de abril de 2006
 se utiliza para almacenar, procesar y analizar grandes volúmenes de datos
Flume	Apache	Comienza a desarrollarse en 20010 (poco después que Scribe por Facebook), como servidor de logs, y pasa a la incubadora como proyecto Apache en el 2010 (v.1.0.0), en Julio de 2012 se convierte en proyecto Top (v.1.2.0).	Es un sistema distribuido, confiable y disponible para recoger, agregar y mover grandes cantidades de datos en logs desde diferentes orígenes a un almacén centralizado. También sirve para  los orígenes son personalizables, Flume puede transportar gran cantidad de eventos incluyendo datos generados por las redes sociales, tráfico de red, mensajes de correo electrónico y casi cualquier fuente de datos configurable.
Map reduce 			MapReduce es un framework que proporciona un sistema de procesamiento de datos paralelo y distribuido. También es ofrecer una forma simple, rápida, escalable y resistente a fallos para manipular enormes cantidades de datos. 
Base de datos no estructurados 			Generalmente son datos binarios que no tienen estructura interna identificable. Es un conglomerado masivo y desorganizado de varios objetos que no tienen valor hasta que se identifican y almacenan de manera organizada
Docker	Solomon Hykes comenzó Docker como un proyecto interno dentro dotCloud, empresa enfocada a una plataforma como un servicio (PaaS), con las contribuciones iniciales de otros ingenieros de dotCloud, incluyendo Andrea Luzzardi y Francois-Xavier Bourlet.	20 de marzo de 2013	es una herramienta diseñada para beneficiar tanto a desarrolladores, testers, como administradores de sistemas, en relación a las máquinas, a los entornos en sí donde se ejecutan las aplicaciones software, los procesos de despliegue, etc
MONGO DB	Magnusson y Dwight Merriman	11 de febrero de 2009
	MongoDB es la base de datos NoSQL líder y permite a las empresas ser más ágiles y escalables. Organizaciones de todos los tamaños están usando MongoDB para crear nuevos tipos de aplicaciones, mejorar la experiencia del cliente, acelerar el tiempo de comercialización y reducir costes
Spark	 Matei Zaharia en el AMPLab de la UC Berkeley en 2009.	Apache Spark nació en 2009 en la Universidad de Berkeley	Apache Spark es un sistema de computación que se basa en Hadoop Map Reduce y que, principalmente, permite dividir o paralelizar el trabajo, ya que normalmente se instala en un clúster de máquina. La idea es que tengamos n máquinas, por ejemplo diez máquinas, y cada una de esas instancias va a tener instalada una versión de Apache Spark.


